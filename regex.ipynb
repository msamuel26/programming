{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hari ini mempelajari regex, contoh untuk mempelajari regex yaitu \"https://regex101.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['9', '9', '1', '0', '1', '2', '1', '2', '8', '3', '8', '8', '2', '8', '3', '8', '8', '2', '9', '0']\n",
      " \n",
      "['m', 'o', 's', 'e', 's', 'p', 'a', 's', 't', 'i', 'b', 'i', 's', 'a']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "string = 'm 9 o 9 s 10 e 12 s 12 p 83 a 88 s 283 ti 88 b 2 i 90 sa'\n",
    "pattern = '\\d' # mencari data hanya berupa angka/digit\n",
    "pattern1 = '[a-z]' # mencari data dari huruf a sampai z\n",
    "\n",
    "matches = re.findall(pattern, string)\n",
    "print(matches)\n",
    "print(\" \")\n",
    "\n",
    "matches1 = re.findall(pattern1, string)\n",
    "print(matches1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada \"https://regex101.com/\", terdapat kolom untuk mengisi kode pattern yang dimana kita bisa mengetahui artinya ketika kita mengarahkan kursor pada kode pattern tersebut.\n",
    "\n",
    "Contoh jika kita mengisi \"\\d\" pada kolom tersebut, maka terdapat penjelasan yaitu \"matches a digit (equivalent to [0-9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "686\n"
     ]
    }
   ],
   "source": [
    "string = 'm 9 o 9 s 10 e 12 s 12 p 83 a 88 s 283 ti 88 b 2 i 90 sa'\n",
    "a, result = string.split(' '), 0\n",
    "for value in a:\n",
    "    if value.isdigit():\n",
    "        result += int(value)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "\n",
    "url = 'https://mrp.kanalseng.com/'\n",
    "response = requests.get(url)\n",
    "html = response.content.decode('utf-8')\n",
    "\n",
    "emails = re.findall('/\\w+@[\\w|\\-]+[\\.\\w+]+/gm', html)\n",
    "print(emails) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hari ini mempelajari Web Scraping menggunakan Regex pada Python,\n",
    "dibawah ini akan mengambil informasi nama headers/judul dari masing-masing tabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_largest_companies_in_the_United_States_by_revenue'\n",
    "\n",
    "page = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(page.text, 'html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<table class=\"box-More_citations_needed plainlinks metadata ambox ambox-content ambox-Refimprove\" role=\"presentation\"><tbody><tr><td class=\"mbox-image\"><div class=\"mbox-image-div\"><span typeof=\"mw:File\"><a class=\"mw-file-description\" href=\"/wiki/File:Question_book-new.svg\"><img alt=\"\" class=\"mw-file-element\" data-file-height=\"399\" data-file-width=\"512\" decoding=\"async\" height=\"39\" src=\"//upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/50px-Question_book-new.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/75px-Question_book-new.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/100px-Question_book-new.svg.png 2x\" width=\"50\"/></a></span></div></td><td class=\"mbox-text\"><div class=\"mbox-text-span\">This article <b>needs additional citations for <a href=\"/wiki/Wikipedia:Verifiability\" title=\"Wikipedia:Verifiability\">verification</a></b>.<span class=\"hide-when-compact\"> Please help <a href=\"/wiki/Special:EditPage/List_of_largest_companies_in_the_United_States_by_revenue\" title=\"Special:EditPage/List of largest companies in the United States by revenue\">improve this article</a> by <a href=\"/wiki/Help:Referencing_for_beginners\" title=\"Help:Referencing for beginners\">adding citations to reliable sources</a>. Unsourced material may be challenged and removed.<br/><small><span class=\"plainlinks\"><i>Find sources:</i> <a class=\"external text\" href=\"https://www.google.com/search?as_eq=wikipedia&amp;q=%22List+of+largest+companies+in+the+United+States+by+revenue%22\" rel=\"nofollow\">\"List of largest companies in the United States by revenue\"</a> – <a class=\"external text\" href=\"https://www.google.com/search?tbm=nws&amp;q=%22List+of+largest+companies+in+the+United+States+by+revenue%22+-wikipedia&amp;tbs=ar:1\" rel=\"nofollow\">news</a> <b>·</b> <a class=\"external text\" href=\"https://www.google.com/search?&amp;q=%22List+of+largest+companies+in+the+United+States+by+revenue%22&amp;tbs=bkt:s&amp;tbm=bks\" rel=\"nofollow\">newspapers</a> <b>·</b> <a class=\"external text\" href=\"https://www.google.com/search?tbs=bks:1&amp;q=%22List+of+largest+companies+in+the+United+States+by+revenue%22+-wikipedia\" rel=\"nofollow\">books</a> <b>·</b> <a class=\"external text\" href=\"https://scholar.google.com/scholar?q=%22List+of+largest+companies+in+the+United+States+by+revenue%22\" rel=\"nofollow\">scholar</a> <b>·</b> <a class=\"external text\" href=\"https://www.jstor.org/action/doBasicSearch?Query=%22List+of+largest+companies+in+the+United+States+by+revenue%22&amp;acc=on&amp;wc=on\" rel=\"nofollow\">JSTOR</a></span></small></span> <span class=\"date-container\"><i>(<span class=\"date\">June 2020</span>)</i></span><span class=\"hide-when-compact\"><i> (<small><a href=\"/wiki/Help:Maintenance_template_removal\" title=\"Help:Maintenance template removal\">Learn how and when to remove this template message</a></small>)</i></span></div></td></tr></tbody></table>\n"
     ]
    }
   ],
   "source": [
    "x = soup.find('table')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.find_all('table')[1]\n",
    "print(table)\n",
    "\n",
    "# Output:\n",
    "\n",
    "# <table class=\"wikitable sortable\">\n",
    "# <caption>\n",
    "# </caption>\n",
    "# <tbody><tr>\n",
    "# <th>Rank\n",
    "# </th>\n",
    "# <th>Name\n",
    "# </th>\n",
    "# <th>Industry\n",
    "# </th>\n",
    "# <th>Revenue <br/>(USD millions)\n",
    "# </th>\n",
    "# <th>Revenue growth\n",
    "# </th>\n",
    "# <th>Employees\n",
    "# </th>\n",
    "# <th>Headquarters\n",
    "# </th></tr>\n",
    "# <tr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<th>Rank\n",
      "</th>, <th>Name\n",
      "</th>, <th>Industry\n",
      "</th>, <th>Revenue <br/>(USD millions)\n",
      "</th>, <th>Revenue growth\n",
      "</th>, <th>Employees\n",
      "</th>, <th>Headquarters\n",
      "</th>] \n",
      "\n",
      "['Rank', 'Name', 'Industry', 'Revenue (USD millions)', 'Revenue growth', 'Employees', 'Headquarters']\n"
     ]
    }
   ],
   "source": [
    "# Untuk mengambil data judul/header dari tabel diinginkan yang berada di \"https://en.wikipedia.org/wiki/List_of_largest_companies_in_the_United_States_by_revenue\"\n",
    "get_titles = table.find_all('th')\n",
    "print(get_titles, \"\\n\")\n",
    "\n",
    "# Untuk mendapatkan data header dari tabel pertama (menjadi lebih rapi)\n",
    "title_headers = [value.text.strip() for value in get_titles]\n",
    "print(title_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rank', 'Name', 'Industry', 'Revenue (USD millions)', 'Revenue growth', 'Employees', 'Headquarters'] \n",
      "\n",
      "['Rank', 'Name', 'Industry', 'Revenue (USD billions)', 'Employees', 'Headquarters'] \n",
      "\n",
      "['Rank', 'Name', 'Industry', 'Profits(USD millions)']\n"
     ]
    }
   ],
   "source": [
    "# Mengambil data headers dari tabel pertama\n",
    "first_table = soup.find_all('table')[1]\n",
    "titles1 = first_table.find_all('th')\n",
    "\n",
    "result1 = [value.text.strip() for value in titles1]\n",
    "print(result1, \"\\n\")\n",
    "\n",
    "# Mengambil data headers dari tabel kedua\n",
    "second_table = soup.find_all('table')[2]\n",
    "titles2 = second_table.find_all('th')\n",
    "\n",
    "result2 = [data.text.strip() for data in titles2]\n",
    "print(result2, \"\\n\")\n",
    "\n",
    "# Mengambil data headers dari tabel ketiga\n",
    "third_table = soup.find_all('table')[3]\n",
    "titles3 = third_table.find_all('th')\n",
    "\n",
    "result3 = [value.text.strip() for value in titles3]\n",
    "print(result3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hari ini mempelajari lagi mengenai Web Scraping (mengambil data dari website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Disini kita akan mengambil semua data \"h2\" dalam website/URL di bawah ini (tanpa mengambil \"Contents\")\n",
    "url = 'https://en.wikipedia.org/wiki/Economy_of_China'\n",
    "\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<span class=\"mw-headline\" id=\"History\">History</span>\n",
      "<span class=\"mw-headline\" id=\"Wealth_in_China\">Wealth in China</span>\n",
      "<span class=\"mw-headline\" id=\"Regional_economies_for_China\">Regional economies for China</span>\n",
      "<span class=\"mw-headline\" id=\"Development\">Development</span>\n",
      "<span class=\"mw-headline\" id=\"Macroeconomic_trends\">Macroeconomic trends</span>\n",
      "<span class=\"mw-headline\" id=\"Macroeconomic_issues\">Macroeconomic issues</span>\n",
      "<span class=\"mw-headline\" id=\"Financial_and_banking_system\">Financial and banking system</span>\n",
      "<span class=\"mw-headline\" id=\"Sectors\">Sectors</span>\n",
      "<span class=\"mw-headline\" id=\"Labor_and_welfare\">Labor and welfare</span>\n",
      "<span class=\"mw-headline\" id=\"State-owned_enterprises\">State-owned enterprises</span>\n",
      "<span class=\"mw-headline\" id=\"External_trade\">External trade</span>\n",
      "<span class=\"mw-headline\" id=\"Foreign_investment\">Foreign investment</span>\n",
      "<span class=\"mw-headline\" id=\"Demographics\">Demographics</span>\n",
      "<span class=\"mw-headline\" id=\"Transportation_and_infrastructure\">Transportation and infrastructure</span>\n",
      "<span class=\"mw-headline\" id=\"Science_and_technology\">Science and technology</span>\n",
      "<span class=\"mw-headline\" id=\"Anti-monopoly_and_competition\">Anti-monopoly and competition</span>\n",
      "<span class=\"mw-headline\" id=\"See_also\">See also</span>\n",
      "<span class=\"mw-headline\" id=\"Notes\">Notes</span>\n",
      "<span class=\"mw-headline\" id=\"References\">References</span>\n",
      "<span class=\"mw-headline\" id=\"Further_reading\">Further reading</span>\n",
      " \n",
      "Final result: ['History', 'Wealth in China', 'Regional economies for China', 'Development', 'Macroeconomic trends', 'Macroeconomic issues', 'Financial and banking system', 'Sectors', 'Labor and welfare', 'State-owned enterprises', 'External trade', 'Foreign investment', 'Demographics', 'Transportation and infrastructure', 'Science and technology', 'Anti-monopoly and competition', 'See also', 'Notes', 'References', 'Further reading']\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "x = soup.find_all('h2')\n",
    "for h2 in x:\n",
    "    span = h2.find('span')\n",
    "    print(span)\n",
    "    if span:\n",
    "        result.append(span.text)\n",
    "\n",
    "print(\" \")\n",
    "print(\"Final result:\", result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
